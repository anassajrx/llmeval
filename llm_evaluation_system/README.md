# LLM Evaluation System

Un systÃ¨me complet d'Ã©valuation des modÃ¨les de langage (LLM) basÃ© sur des QCM gÃ©nÃ©rÃ©s automatiquement, spÃ©cialisÃ© pour le domaine juridique.

## ğŸŒŸ CaractÃ©ristiques

- **Traitement de Documents**
  - Extraction de texte Ã  partir de PDF
  - DÃ©coupage intelligent en chunks
  - GÃ©nÃ©ration d'embeddings avec Gemini

- **GÃ©nÃ©ration de QCM**
  - Questions contextualisÃ©es
  - Multiple critÃ¨res d'Ã©valuation
  - Niveaux de difficultÃ© variables

- **Ã‰valuation AvancÃ©e**
  - Tests de biais
  - VÃ©rification d'intÃ©gritÃ©
  - ConformitÃ© lÃ©gale
  - Analyse de cohÃ©rence

- **Rapports DÃ©taillÃ©s**
  - Visualisations interactives
  - MÃ©triques avancÃ©es
  - Exportation multi-format

## ğŸ“‹ PrÃ©requis

- Python 3.8+
- Base de donnÃ©es PostgreSQL avec extension pgvector
- ClÃ© API Gemini
- Environnement virtuel Python (recommandÃ©)

## ğŸš€ Installation

1. Cloner le repository :
```bash
git clone https://github.com/votre-username/llm-evaluation-system.git
cd llm-evaluation-system
```

2. CrÃ©er et activer l'environnement virtuel :
```bash
python -m venv venv
source venv/bin/activate  # Sur Unix/MacOS
# ou
venv\Scripts\activate  # Sur Windows
```

3. Installer les dÃ©pendances :
```bash
pip install -r requirements.txt
```

4. Configurer les clÃ©s API :
- CrÃ©er un fichier `.env.apikey` dans `AI_API/API_Calls/GeminiProAPI/settings/`
- CrÃ©er un fichier `.env.apikey` dans `AI_API/API_Calls/PostgresURL/settings/`

## ğŸ“Š Utilisation

1. Placer les documents PDF Ã  analyser dans le dossier `data/input/`

2. ExÃ©cuter le systÃ¨me :
```bash
python main.py
```

3. Consulter les rÃ©sultats dans le dossier `data/output/`

## ğŸ“ Structure du Projet

```
llm_evaluation_system/
â”œâ”€â”€ config/                 # Configuration
â”œâ”€â”€ core/                   # Composants principaux
â”œâ”€â”€ generators/             # GÃ©nÃ©rateurs (QCM, rapports)
â”œâ”€â”€ evaluators/            # SystÃ¨mes d'Ã©valuation
â”œâ”€â”€ utils/                 # Utilitaires
â”œâ”€â”€ tests/                 # Tests unitaires
â””â”€â”€ data/                  # DonnÃ©es
    â”œâ”€â”€ input/             # Documents d'entrÃ©e
    â””â”€â”€ output/            # RÃ©sultats et rapports
```

## ğŸ” DÃ©tails Techniques

### Traitement des Documents
- Utilisation de PyMuPDF pour l'extraction de texte
- Chunking avec chevauchement pour maintenir le contexte
- Embeddings via l'API Gemini

### GÃ©nÃ©ration de QCM
- Questions basÃ©es sur le contexte extrait
- 5 critÃ¨res d'Ã©valuation principaux
- 3 niveaux de difficultÃ© par critÃ¨re

### Ã‰valuation
- Tests de rÃ©sistance aux biais
- VÃ©rification de cohÃ©rence
- Analyse de conformitÃ© lÃ©gale
- MÃ©triques de performance dÃ©taillÃ©es

## ğŸ“ˆ MÃ©triques d'Ã‰valuation

- Score global
- Scores par critÃ¨re
- Taux de rÃ©ussite
- RÃ©sistance aux biais
- IntÃ©gritÃ© des rÃ©ponses
- ConformitÃ© lÃ©gale
- CohÃ©rence logique

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Veuillez :
1. Fork le projet
2. CrÃ©er une branche pour votre fonctionnalitÃ©
3. Commiter vos changements
4. Pousser vers la branche
5. Ouvrir une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ‘¥ Auteurs

- Votre Nom (@votre-username)

## ğŸ“§ Contact

Pour toute question ou suggestion, veuillez ouvrir une issue dans le repository GitHub.

## ğŸ™ Remerciements

- Anthropic pour l'API Claude
- Google pour l'API Gemini
- La communautÃ© open source pour les diffÃ©rentes bibliothÃ¨ques utilisÃ©es
```

Ceci termine la structure de base du projet. Je peux maintenant :
1. CrÃ©er un script d'installation
2. Ajouter des tests unitaires
3. AmÃ©liorer la documentation
4. Ajouter des exemples d'utilisation

Que souhaitez-vous que je fasse ensuite ?